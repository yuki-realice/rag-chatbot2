#!/usr/bin/env python3
"""
Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨è¡Œã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä¿å­˜çŠ¶æ³ã®åŒ…æ‹¬çš„æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import os
import pandas as pd
from pathlib import Path

# backend ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ import ã™ã‚‹ãŸã‚ã®ãƒ‘ã‚¹è¨­å®š
backend_dir = Path(__file__).resolve().parent / "backend"
sys.path.insert(0, str(backend_dir))

try:
    from backend.retriever import EnhancedRetriever
    from backend.config import Config
    from backend.ingest_excel import ExcelIngestor
    from backend.rag_service import RAGService
    from backend.utils.name_normalize import normalize_name, build_name_variants
    print("âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«æˆåŠŸã—ã¾ã—ãŸ")
except ImportError as e:
    print(f"âŒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    sys.exit(1)

def analyze_excel_file():
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°åˆ†æ"""
    excel_file = "/Users/yuki/Desktop/rag chatbot2/backend/data/docs/ragç”¨_æ¶é›»ãƒªã‚¹ãƒˆ.xlsx"
    
    print("=" * 60)
    print("Excelãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°åˆ†æ")
    print("=" * 60)
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«åŸºæœ¬æƒ…å ±
        path = Path(excel_file)
        if not path.exists():
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {excel_file}")
            return None, None
        
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {path.name}")
        print(f"ğŸ“Š ã‚µã‚¤ã‚º: {path.stat().st_size / 1024:.1f} KB")
        
        # Excelèª­ã¿è¾¼ã¿ï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
        xl_file = pd.ExcelFile(str(path), engine='openpyxl')
        sheet_names = xl_file.sheet_names
        print(f"ğŸ“‹ ã‚·ãƒ¼ãƒˆ: {sheet_names}")
        
        # æœ€åˆã®ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
        sheet_name = sheet_names[0]
        df_raw = pd.read_excel(str(path), sheet_name=sheet_name, engine='openpyxl', dtype=str, header=None)
        df_raw = df_raw.fillna("")
        
        print(f"ğŸ“ ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df_raw.shape[0]} è¡Œ Ã— {df_raw.shape[1]} åˆ—")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†ï¼ˆExcelIngestorã¨åŒã˜å‡¦ç†ï¼‰
        if len(df_raw) < 2:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
            return None, None
            
        header_row = df_raw.iloc[0].tolist()
        print(f"ğŸ·ï¸  ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ: {header_row[:8]}...")
        
        # å‡¦ç†æ¸ˆã¿ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        processed_header = []
        for i, cell_value in enumerate(header_row):
            if cell_value and cell_value.strip():
                processed_header.append(cell_value.strip())
            else:
                col_letter = chr(ord('A') + i)
                processed_header.append(f"åˆ—{col_letter}")
        
        # ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†å–å¾—
        df = df_raw.iloc[1:].copy()
        df.columns = processed_header
        
        # åˆ—åæ¨™æº–åŒ–
        df = df.rename(columns={
            "ä¼æ¥­å": "company_name",
            "ä¼šç¤¾å": "company_name", 
            "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "lead_status",
            "ãƒªãƒ¼ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "lead_status"
        })
        
        print(f"ğŸ“Š å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(df)} è¡Œ")
        print(f"ğŸ“‹ åˆ—å: {list(df.columns)[:8]}...")
        
        return df, processed_header
        
    except Exception as e:
        print(f"âŒ Excelãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def analyze_data_processing(df):
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®è©³ç´°åˆ†æï¼ˆExcelIngestorã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰"""
    print("\n" + "=" * 60)
    print("ãƒ‡ãƒ¼ã‚¿å‡¦ç†åˆ†æ")
    print("=" * 60)
    
    if df is None:
        return {}
    
    # row_idåˆ—ã‚’ä»˜ä¸ï¼ˆExcelIngestorã¨åŒã˜å‡¦ç†ï¼‰
    df = df.reset_index().assign(row_id=lambda d: d.index + 2)
    
    columns = list(df.columns)
    
    # ä¼æ¥­ååˆ—ã¨ãƒªãƒ¼ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ã‚’ç‰¹å®š
    company_col = None
    lead_status_col = None
    
    if "company_name" in columns:
        company_col = "company_name"
    else:
        company_col = columns[0] if len(columns) > 0 else None
        
    if "lead_status" in columns:
        lead_status_col = "lead_status"
    else:
        lead_status_col = columns[6] if len(columns) > 6 else None
    
    print(f"ğŸ¢ ä¼æ¥­ååˆ—: '{company_col}'")
    print(f"ğŸ“Š ãƒªãƒ¼ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—: '{lead_status_col}'")
    
    # å„è¡Œã®å‡¦ç†çŠ¶æ³ã‚’åˆ†æ
    valid_rows = 0
    skipped_rows = 0
    skipped_reasons = {}
    valid_companies = set()
    
    print(f"\nğŸ“ è¡Œåˆ¥å‡¦ç†çŠ¶æ³åˆ†æ:")
    print(f"{'è¡Œç•ªå·':<6} {'ä¼æ¥­å':<30} {'å‡¦ç†çŠ¶æ³':<15} {'ç†ç”±'}")
    print("-" * 80)
    
    for idx, row in df.iterrows():
        excel_row_num = int(row['row_id'])
        company = str(row[company_col]).strip() if company_col else ""
        
        # ExcelIngestorã¨åŒã˜æ¡ä»¶ã§ãƒã‚§ãƒƒã‚¯
        skip_reason = None
        
        if not company or company.lower() in ['nan', 'none', ''] or company == company_col:
            skip_reason = "ä¼æ¥­åç„¡åŠ¹"
            skipped_rows += 1
            skipped_reasons[skip_reason] = skipped_reasons.get(skip_reason, 0) + 1
        else:
            valid_rows += 1
            valid_companies.add(company)
            skip_reason = "å‡¦ç†æˆåŠŸ"
        
        # æœ€åˆã®50è¡Œã®è©³ç´°è¡¨ç¤º
        if excel_row_num <= 51:  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ+50è¡Œ
            status = "âœ… å‡¦ç†" if skip_reason == "å‡¦ç†æˆåŠŸ" else "âŒ ã‚¹ã‚­ãƒƒãƒ—"
            print(f"{excel_row_num:<6} {company[:28]:<30} {status:<15} {skip_reason}")
    
    print(f"\nğŸ“Š å‡¦ç†çµ±è¨ˆ:")
    print(f"  ç·ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(df)}")
    print(f"  æœ‰åŠ¹è¡Œæ•°: {valid_rows}")
    print(f"  ã‚¹ã‚­ãƒƒãƒ—è¡Œæ•°: {skipped_rows}")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°: {len(valid_companies)}")
    
    if skipped_reasons:
        print(f"\nâŒ ã‚¹ã‚­ãƒƒãƒ—ç†ç”±åˆ¥çµ±è¨ˆ:")
        for reason, count in skipped_reasons.items():
            print(f"  {reason}: {count}è¡Œ")
    
    return {
        "total_rows": len(df),
        "valid_rows": valid_rows,
        "skipped_rows": skipped_rows, 
        "unique_companies": len(valid_companies),
        "companies": valid_companies,
        "skipped_reasons": skipped_reasons
    }

def analyze_vectorstore():
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®å†…å®¹åˆ†æ"""
    print("\n" + "=" * 60)
    print("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å†…å®¹åˆ†æ")
    print("=" * 60)
    
    try:
        retriever = EnhancedRetriever()
        collection = retriever.vectorstore._collection
        
        # ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        total_count = collection.count()
        print(f"ğŸ“Š ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_count}")
        
        if total_count == 0:
            print("âŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒç©ºã§ã™")
            return {}
        
        # å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—
        all_docs = collection.get(
            limit=total_count,
            include=["metadatas"]
        )
        
        # ä¼æ¥­åˆ¥çµ±è¨ˆ
        companies = set()
        row_ids = set()
        lead_statuses = {}
        
        if all_docs and all_docs.get("metadatas"):
            for meta in all_docs["metadatas"]:
                company = meta.get("company", "")
                row_id = meta.get("row_id", "")
                lead_status = meta.get("lead_status", "")
                
                if company:
                    companies.add(company)
                if row_id:
                    row_ids.add(row_id)
                if lead_status:
                    lead_statuses[lead_status] = lead_statuses.get(lead_status, 0) + 1
        
        print(f"ğŸ“ˆ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢çµ±è¨ˆ:")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°: {len(companies)}")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯è¡ŒIDæ•°: {len(row_ids)}")
        print(f"  è¡ŒIDç¯„å›²: {min(row_ids) if row_ids else 'N/A'} - {max(row_ids) if row_ids else 'N/A'}")
        
        print(f"\nğŸ“Š ãƒªãƒ¼ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ†å¸ƒ:")
        for status, count in sorted(lead_statuses.items()):
            print(f"  {status}: {count}ä»¶")
        
        return {
            "total_documents": total_count,
            "unique_companies": len(companies),
            "unique_rows": len(row_ids),
            "companies": companies,
            "row_ids": row_ids,
            "lead_statuses": lead_statuses
        }
        
    except Exception as e:
        print(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return {}

def test_daiou_package_search():
    """å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ ªå¼ä¼šç¤¾ã®æ¤œç´¢è©³ç´°ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 60)
    print("å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ ªå¼ä¼šç¤¾ æ¤œç´¢è©³ç´°ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        retriever = EnhancedRetriever()
        collection = retriever.vectorstore._collection
        
        # 1. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ã€Œå¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã€ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç›´æ¥ç¢ºèª
        print("ğŸ” 1. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ç›´æ¥ç¢ºèª:")
        all_docs = collection.get(
            limit=1000,
            include=["metadatas", "documents"]
        )
        
        daiou_found = False
        if all_docs and all_docs.get("metadatas"):
            for i, meta in enumerate(all_docs["metadatas"]):
                company = meta.get("company", "")
                if "å¤§ç‹" in company or "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸" in company:
                    daiou_found = True
                    doc_content = all_docs["documents"][i] if i < len(all_docs["documents"]) else ""
                    print(f"  âœ… ç™ºè¦‹: {company}")
                    print(f"     è¡ŒID: {meta.get('row_id', 'N/A')}")
                    print(f"     ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {meta.get('lead_status', 'N/A')}")
                    print(f"     æ­£è¦åŒ–å: {meta.get('company_name_norm', 'N/A')}")
                    print(f"     ãƒãƒªã‚¢ãƒ³ãƒˆ: {meta.get('company_name_variants', 'N/A')[:100]}...")
                    print(f"     å†…å®¹: {doc_content[:100]}...")
        
        if not daiou_found:
            print("  âŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ã€Œå¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã€ã‚’å«ã‚€ä¼æ¥­ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # 2. ä¼æ¥­åæ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ”§ 2. ä¼æ¥­åæ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ:")
        test_names = [
            "å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ ªå¼ä¼šç¤¾",
            "å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸",
            "ãƒ€ã‚¤ã‚ªã‚¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ ªå¼ä¼šç¤¾",
            "ãƒ€ã‚¤ã‚ªã‚¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸"
        ]
        
        for name in test_names:
            normalized = normalize_name(name)
            variants = build_name_variants(name)
            print(f"  '{name}'")
            print(f"    æ­£è¦åŒ–: '{normalized}'")
            print(f"    ãƒãƒªã‚¢ãƒ³ãƒˆ: {variants[:3]}...")
        
        # 3. å„ç¨®æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ” 3. å„ç¨®æ¤œç´¢ãƒ†ã‚¹ãƒˆ:")
        search_queries = [
            "å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ ªå¼ä¼šç¤¾",
            "å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸", 
            "ãƒ€ã‚¤ã‚ªã‚¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸",
            "å¤§ç‹",
            "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸"
        ]
        
        for query in search_queries:
            print(f"\n  ã‚¯ã‚¨ãƒª: '{query}'")
            
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
            try:
                results = retriever.hybrid_search(query, top_k=3)
                print(f"    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢çµæœ: {len(results)}ä»¶")
                for i, doc in enumerate(results[:2]):
                    company = doc.metadata.get("company", "N/A")
                    status = doc.metadata.get("lead_status", "N/A")
                    print(f"      {i+1}. ä¼æ¥­: {company}, ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")
            except Exception as e:
                print(f"    âŒ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿
            try:
                vector_results = retriever._vector_search(query, None, None, 3)
                print(f"    ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ: {len(vector_results)}ä»¶")
                for i, doc in enumerate(vector_results[:2]):
                    company = doc.metadata.get("company", "N/A")
                    print(f"      {i+1}. ä¼æ¥­: {company}")
            except Exception as e:
                print(f"    âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 4. RAGãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ’¬ 4. RAGãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ:")
        try:
            rag_service = RAGService()
            chat_queries = [
                "å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ ªå¼ä¼šç¤¾ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
                "å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®æƒ…å ±ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
            ]
            
            for query in chat_queries:
                print(f"\n  è³ªå•: '{query}'")
                response = rag_service.chat(query)
                
                if response.get("status") == "success":
                    answer = response.get("answer", "")
                    sources = response.get("sources", [])
                    print(f"    âœ… å›ç­”: {answer[:200]}...")
                    print(f"    ğŸ“š æƒ…å ±æºæ•°: {len(sources)}")
                    for source in sources[:2]:
                        print(f"       - {source.get('source', 'N/A')[:100]}...")
                else:
                    print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {response.get('message', 'N/A')}")
                    
        except Exception as e:
            print(f"    âŒ RAGãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_results(excel_stats, vectorstore_stats):
    """Excelã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ãƒ‡ãƒ¼ã‚¿Sæ¯”è¼ƒ"""
    print("\n" + "=" * 60)
    print("ãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒåˆ†æ")
    print("=" * 60)
    
    if not excel_stats or not vectorstore_stats:
        print("âŒ æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        return
    
    excel_companies = excel_stats.get("companies", set())
    vector_companies = vectorstore_stats.get("companies", set())
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒ:")
    print(f"  Excelæœ‰åŠ¹è¡Œæ•°: {excel_stats.get('valid_rows', 0)}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢è¡Œæ•°: {vectorstore_stats.get('unique_rows', 0)}")
    print(f"  Excelãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°: {len(excel_companies)}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°: {len(vector_companies)}")
    
    # ä¸€è‡´ç‡è¨ˆç®—
    if excel_companies and vector_companies:
        common_companies = excel_companies & vector_companies
        missing_in_vector = excel_companies - vector_companies
        extra_in_vector = vector_companies - excel_companies
        
        match_rate = len(common_companies) / len(excel_companies) * 100
        
        print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ä¸€è‡´æ€§:")
        print(f"  ä¸€è‡´ä¼æ¥­æ•°: {len(common_companies)}")
        print(f"  ä¸€è‡´ç‡: {match_rate:.1f}%")
        
        if missing_in_vector:
            print(f"\nâŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«æœªä¿å­˜ã®ä¼æ¥­ ({len(missing_in_vector)}ç¤¾):")
            for company in sorted(list(missing_in_vector)[:10]):
                print(f"    - {company}")
            if len(missing_in_vector) > 10:
                print(f"    ... ä»– {len(missing_in_vector) - 10}ç¤¾")
        
        if extra_in_vector:
            print(f"\nâš ï¸  Excelã«ãªã„ä¼æ¥­ãŒãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«å­˜åœ¨ ({len(extra_in_vector)}ç¤¾):")
            for company in sorted(list(extra_in_vector)[:5]):
                print(f"    - {company}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ” Excel-ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢åŒ…æ‹¬æ¤œè¨¼ãƒ„ãƒ¼ãƒ«")
    print()
    
    # 1. Excelãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
    df, header = analyze_excel_file()
    
    # 2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†åˆ†æ  
    excel_stats = analyze_data_processing(df)
    
    # 3. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢åˆ†æ
    vectorstore_stats = analyze_vectorstore()
    
    # 4. æ¯”è¼ƒåˆ†æ
    compare_results(excel_stats, vectorstore_stats)
    
    # 5. å¤§ç‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ¤œç´¢è©³ç´°ãƒ†ã‚¹ãƒˆ
    test_daiou_package_search()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ åŒ…æ‹¬æ¤œè¨¼å®Œäº†")
    print("=" * 60)

if __name__ == "__main__":
    main()
